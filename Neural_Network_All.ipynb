{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Model-creation\" data-toc-modified-id=\"Model-creation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Model creation</a></span></li><li><span><a href=\"#Data-Importation\" data-toc-modified-id=\"Data-Importation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Importation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Text\" data-toc-modified-id=\"Text-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Text</a></span></li><li><span><a href=\"#Images\" data-toc-modified-id=\"Images-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Images</a></span></li></ul></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Training</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Flatten, Embedding\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.applications import vgg16\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import layers, models, Model, optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import concatenate\n",
    "import keras\n",
    "import numpy as np\n",
    "import argparse\n",
    "import locale\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential()\n",
    "cnn.add(layers.Flatten(input_shape=(28,28))) # First layer to flatten\n",
    "cnn.add(layers.Dense(128, activation='relu')) # Second layer with Relu activation\n",
    "cnn.add(layers.Dropout(0.2)) # Third layer to reduct the dimension\n",
    "cnn.add(layers.Dense(10, activation='relu')) # We then stack Dense layer\n",
    "cnn.add(layers.Dense(4, activation='relu')) # And the last one to get the 4 output\n",
    "\n",
    "emb = Sequential()\n",
    "emb.add(layers.Dense(10, input_dim=100, activation='relu'))\n",
    "emb.add(layers.Dense(10, activation='relu'))\n",
    "emb.add(layers.Dense(10, activation='relu'))\n",
    "emb.add(layers.Dense(4, activation='relu'))\n",
    "\n",
    "combinedInput = concatenate([emb.output, \n",
    "                             cnn.output])\n",
    "\n",
    "# We add the final layer (the very last one is up to you, reg or classifier)\n",
    "x = Dense(4, activation='relu')(combinedInput)\n",
    "\n",
    "# As said, the last layer is up to you, but here we add a classifier for 7 class\n",
    "x = Dense(7, activation='softmax')(x)\n",
    "\n",
    "# And we build the model\n",
    "model = Model(inputs=[emb.input, cnn.input], outputs=x)\n",
    "\n",
    "# We define the number of epoch and the validation steps\n",
    "epochs = 10\n",
    "validation_steps = 2\n",
    "\n",
    "# We compile the model\n",
    "opt = Adam(lr=1e-5, clipnorm = 1.)\n",
    "loss_fn  = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(loss=loss_fn, optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "flatten_2_input (InputLayer)    (None, 28, 28)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_13_input (InputLayer)     (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 784)          0           flatten_2_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 10)           1010        dense_13_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 128)          100480      flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 10)           110         dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 10)           110         dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 10)           1290        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 4)            44          dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 4)            44          dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 8)            0           dense_16[0][0]                   \n",
      "                                                                 dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 4)            36          concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 7)            35          dense_17[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 103,159\n",
      "Trainable params: 103,159\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Importation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/data.csv')\n",
    "target = pd.read_csv('data/target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_text = data[data['label'] == 'Train']['description']\n",
    "x_val_text = data[data['label'] == 'Validation']['description']\n",
    "x_test_text = data[data['label'] == 'Test']['description']\n",
    "\n",
    "target.categ = pd.Categorical(target.categ)\n",
    "target['categ_number'] = np.int32(target.categ.cat.codes)\n",
    "\n",
    "y_train_text = np.asarray(target[target['label'] == 'Train'][['categ_number']])\n",
    "y_val_text = np.asarray(target[target['label'] == 'Validation'][['categ_number']])\n",
    "y_test_text = np.asarray(target[target['label'] == 'Test'][['categ_number']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(x_train_text)\n",
    "\n",
    "X_train_text = tokenizer.texts_to_sequences(x_train_text)\n",
    "X_val_text = tokenizer.texts_to_sequences(x_val_text)\n",
    "X_test_text = tokenizer.texts_to_sequences(x_test_text)\n",
    "\n",
    "maxlen = 100\n",
    "X_train_text = pad_sequences(X_train_text, padding='post', maxlen=maxlen)\n",
    "X_val_text = pad_sequences(X_val_text, padding='post', maxlen=maxlen)\n",
    "X_test_text = pad_sequences(X_test_text, padding='post', maxlen=maxlen)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = \"data/train\"\n",
    "val_data_dir = \"data/validation\"\n",
    "test_data_dir = \"data/test\"\n",
    "category_names = sorted(os.listdir('data/train'))\n",
    "nb_categories = len(category_names)\n",
    "img_height, img_width = 224,224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images for \"training\":\n",
      "Found 630 images belonging to 7 classes.\n",
      "Total number of images for \"validation\":\n",
      "Found 210 images belonging to 7 classes.\n",
      "Total number of images for \"testing\":\n",
      "Found 210 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "#Number of images to load at each iteration\n",
    "batch_size = 32\n",
    "\n",
    "# only rescaling\n",
    "train_datagen =  ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "test_datagen =  ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "# these are generators for train/test data that will read pictures \n",
    "# found in the defined subfolders of 'data/'\n",
    "print('Total number of images for \"training\":')\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "train_data_dir,\n",
    "target_size = (img_height, img_width),\n",
    "class_mode = \"categorical\")\n",
    "\n",
    "print('Total number of images for \"validation\":')\n",
    "val_generator = test_datagen.flow_from_directory(\n",
    "val_data_dir,\n",
    "target_size = (img_height, img_width),\n",
    "class_mode = \"categorical\",\n",
    "shuffle=False)\n",
    "\n",
    "print('Total number of images for \"testing\":')\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "test_data_dir,\n",
    "target_size = (img_height, img_width),\n",
    "class_mode = \"categorical\",\n",
    "shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training model...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "fit_generator() got multiple values for argument 'steps_per_epoch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-7f3035976fcd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m                     \u001b[1;31m#validation_data=([X_val_text, val_generator], y_test_text)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                    )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: fit_generator() got multiple values for argument 'steps_per_epoch'"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] training model...\")\n",
    "model.fit_generator([X_train_text, train_generator], y_train_text,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    steps_per_epoch=32\n",
    "                    #validation_data=([X_val_text, val_generator], y_test_text)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
