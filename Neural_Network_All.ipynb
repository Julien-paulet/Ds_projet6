{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load data</a></span></li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Normalisation\" data-toc-modified-id=\"Normalisation-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Normalisation</a></span></li><li><span><a href=\"#Stemming\" data-toc-modified-id=\"Stemming-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Stemming</a></span></li><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>TF-IDF</a></span></li></ul></li><li><span><a href=\"#Merging-Dataframes\" data-toc-modified-id=\"Merging-Dataframes-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Merging Dataframes</a></span></li><li><span><a href=\"#Building-generators\" data-toc-modified-id=\"Building-generators-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Building generators</a></span></li><li><span><a href=\"#Model-Creation\" data-toc-modified-id=\"Model-Creation-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Model Creation</a></span><ul class=\"toc-item\"><li><span><a href=\"#CNN\" data-toc-modified-id=\"CNN-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>CNN</a></span></li><li><span><a href=\"#LSTM\" data-toc-modified-id=\"LSTM-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>LSTM</a></span></li><li><span><a href=\"#Combining-the-output\" data-toc-modified-id=\"Combining-the-output-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Combining the output</a></span></li><li><span><a href=\"#Compilation\" data-toc-modified-id=\"Compilation-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Compilation</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Training</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\__init__.py:29: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/data.csv\")\n",
    "data = data[['uniq_id', 'description', 'categ', 'path', 'label']].set_index('uniq_id')\n",
    "\n",
    "# We transform the categ into numeric values\n",
    "data.categ = pd.Categorical(data.categ)\n",
    "data['categ_number'] = np.int32(data.categ.cat.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>categ</th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "      <th>categ_number</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uniq_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4b500e244f11a45f5171bd3665413044</th>\n",
       "      <td>Key Features of Jacadi paris Baby Boy's Stripe...</td>\n",
       "      <td>Baby_Care</td>\n",
       "      <td>data/Train/Baby_Care/4b500e244f11a45f5171bd366...</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7ee2e665b7de2e1cc29e3c65bde0c318</th>\n",
       "      <td>Buy Mavi 237MBB Showpiece  -  15 cm for Rs.129...</td>\n",
       "      <td>Home_Decor_And_Festive_Needs</td>\n",
       "      <td>data/Train/Home_Decor_And_Festive_Needs/7ee2e6...</td>\n",
       "      <td>Train</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d21409978f5e404b53f89e444f7893fe</th>\n",
       "      <td>SF by Sonata 7991PP02 Ocean Digital Watch  - F...</td>\n",
       "      <td>Watches</td>\n",
       "      <td>data/Train/Watches/d21409978f5e404b53f89e444f7...</td>\n",
       "      <td>Train</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47d7792e50e69b048b1f17176f170141</th>\n",
       "      <td>Flipkart.com: Buy Beverly Hills Polo Club RED ...</td>\n",
       "      <td>Beauty_and_Personal_Care</td>\n",
       "      <td>data/Train/Beauty_and_Personal_Care/47d7792e50...</td>\n",
       "      <td>Train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ecb5bba9a5984d2b35b8ac3f56056c67</th>\n",
       "      <td>Flipkart.com: Buy Wild Stone Red And Juice Com...</td>\n",
       "      <td>Beauty_and_Personal_Care</td>\n",
       "      <td>data/Train/Beauty_and_Personal_Care/ecb5bba9a5...</td>\n",
       "      <td>Train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        description  \\\n",
       "uniq_id                                                                               \n",
       "4b500e244f11a45f5171bd3665413044  Key Features of Jacadi paris Baby Boy's Stripe...   \n",
       "7ee2e665b7de2e1cc29e3c65bde0c318  Buy Mavi 237MBB Showpiece  -  15 cm for Rs.129...   \n",
       "d21409978f5e404b53f89e444f7893fe  SF by Sonata 7991PP02 Ocean Digital Watch  - F...   \n",
       "47d7792e50e69b048b1f17176f170141  Flipkart.com: Buy Beverly Hills Polo Club RED ...   \n",
       "ecb5bba9a5984d2b35b8ac3f56056c67  Flipkart.com: Buy Wild Stone Red And Juice Com...   \n",
       "\n",
       "                                                         categ  \\\n",
       "uniq_id                                                          \n",
       "4b500e244f11a45f5171bd3665413044                     Baby_Care   \n",
       "7ee2e665b7de2e1cc29e3c65bde0c318  Home_Decor_And_Festive_Needs   \n",
       "d21409978f5e404b53f89e444f7893fe                       Watches   \n",
       "47d7792e50e69b048b1f17176f170141      Beauty_and_Personal_Care   \n",
       "ecb5bba9a5984d2b35b8ac3f56056c67      Beauty_and_Personal_Care   \n",
       "\n",
       "                                                                               path  \\\n",
       "uniq_id                                                                               \n",
       "4b500e244f11a45f5171bd3665413044  data/Train/Baby_Care/4b500e244f11a45f5171bd366...   \n",
       "7ee2e665b7de2e1cc29e3c65bde0c318  data/Train/Home_Decor_And_Festive_Needs/7ee2e6...   \n",
       "d21409978f5e404b53f89e444f7893fe  data/Train/Watches/d21409978f5e404b53f89e444f7...   \n",
       "47d7792e50e69b048b1f17176f170141  data/Train/Beauty_and_Personal_Care/47d7792e50...   \n",
       "ecb5bba9a5984d2b35b8ac3f56056c67  data/Train/Beauty_and_Personal_Care/ecb5bba9a5...   \n",
       "\n",
       "                                  label  categ_number  \n",
       "uniq_id                                                \n",
       "4b500e244f11a45f5171bd3665413044  Train             0  \n",
       "7ee2e665b7de2e1cc29e3c65bde0c318  Train             3  \n",
       "d21409978f5e404b53f89e444f7893fe  Train             6  \n",
       "47d7792e50e69b048b1f17176f170141  Train             1  \n",
       "ecb5bba9a5984d2b35b8ac3f56056c67  Train             1  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will apply some function to the description for modeling later on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = data.reset_index()\n",
    "data_text = data_text[['categ', 'description']]\n",
    "# We take one line per category and sum the description\n",
    "data_text = data_text.groupby('categ').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal here is to extract the frequences of the words\n",
    "# And also to extract the frequences of unique words\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "desc = defaultdict(list)\n",
    "\n",
    "for i in data_text.index:\n",
    "    desc[i] += tokenizer.tokenize(data_text.loc[i, 'description'].lower())\n",
    "\n",
    "stats, freq = dict(), dict()\n",
    "\n",
    "for k, v in zip(desc.keys(), desc.values()):\n",
    "    freq[k] = fq = nltk.FreqDist(v)\n",
    "    stats[k] = {'total': len(v), 'Unique': len(fq.keys())} \n",
    "\n",
    "df = pd.DataFrame.from_dict(stats, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we get the freq of words on our dataset\n",
    "freq_totale = nltk.Counter()\n",
    "for k, v in zip(desc.keys(), desc.values()):\n",
    "    freq_totale += freq[k]\n",
    "\n",
    "# Here is our list of stop words\n",
    "most_freq = { key:value for (key,value) in freq_totale.items() if value >= 100}.keys()\n",
    "\n",
    "# Let's add the common stop words list of nltk\n",
    "sw = set()\n",
    "sw.update(most_freq)\n",
    "sw.update(tuple(nltk.corpus.stopwords.words('english')))\n",
    "sw.update(tuple(['!', '?', '.', ';', '\"', \"'\", \"#\",\n",
    "                 \"&\", \"/\", \"-\", \"_\", \"'m\",\n",
    "                 \"%\", \"*\", \"(\", \")\", \"±3\",\n",
    "                 \",\", \"'s\", '“', '”']))\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "desc = defaultdict(list)\n",
    "\n",
    "for i in data_text.index:\n",
    "    token = tokenizer.tokenize(data_text.loc[i, 'description'].lower())\n",
    "    desc[i] += [w for w in token if not w in list(sw)]\n",
    "\n",
    "stats, freq = dict(), dict()\n",
    "\n",
    "for k, v in zip(desc.keys(), desc.values()):\n",
    "    freq[k] = fq = nltk.FreqDist(v)\n",
    "    stats[k] = {'total': len(v), 'Unique': len(fq.keys())} \n",
    "\n",
    "df = pd.DataFrame.from_dict(stats, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = EnglishStemmer()\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "desc = defaultdict(list)\n",
    "\n",
    "for i in data_text.index:\n",
    "    token = tokenizer.tokenize(data_text.loc[i, 'description'].lower())\n",
    "    desc[i] += [stemmer.stem(w) for w in token if not w in list(sw)]\n",
    "\n",
    "stats, freq = dict(), dict()\n",
    "\n",
    "for k, v in zip(desc.keys(), desc.values()):\n",
    "    freq[k] = fq = nltk.FreqDist(v)\n",
    "    stats[k] = {'total': len(v), 'Unique': len(fq.keys())}\n",
    "\n",
    "df = pd.DataFrame.from_dict(stats, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", '``', 'abov', 'adapt', 'ani', 'babi', 'batteri', 'becaus', 'befor', 'ceram', 'coffe', 'content', 'could', 'deliveri', 'detail', 'dimens', 'discount', 'doe', 'dure', 'easi', 'featur', 'genuin', 'guarante', 'materi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'onlin', 'ourselv', 'packag', 'qualiti', 'replac', 'sale', 'sha', 'ship', 'showpiec', 'specif', 'themselv', 'veri', 'warranti', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "data_tf = data.reset_index()[['uniq_id', 'categ', 'description']]\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "token_dict = defaultdict(list)\n",
    "\n",
    "for i in range(0, len(data_tf)):\n",
    "    lowers = data_tf.loc[i,'description'].lower()\n",
    "    no_punctuation = lowers.translate(string.punctuation)\n",
    "    token_dict[i] = no_punctuation\n",
    "\n",
    "tfIdf = TfidfVectorizer(tokenizer=tokenize, stop_words=sw)\n",
    "values = tfIdf.fit_transform(token_dict.values())\n",
    "\n",
    "#Transform the results into a df\n",
    "values = values.todense()\n",
    "values = pd.DataFrame(values, index=data.index)\n",
    "X_embedded = TSNE(n_components=1).fit_transform(values)\n",
    "X_embedded_df = pd.DataFrame(X_embedded, index=data.index, columns=['tsne'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model = pd.merge(data, values,\n",
    "                      left_index=True, right_index=True,\n",
    "                      how='inner')\n",
    "data_model = data_model.drop(columns=['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the code can be found here : https://github.com/sdcubber/Keras-Sequence-boilerplate/blob/master/Keras-Sequence.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import math\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import Sequence\n",
    "from PIL import Image\n",
    "\n",
    "def load_image(im):\n",
    "    return img_to_array(load_img(im, target_size=(224, 224), grayscale=False)) / 255.\n",
    "\n",
    "class DataSequence(Sequence):\n",
    "    \"\"\"\n",
    "    Keras Sequence object to train a model on larger-than-memory data.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, batch_size, mode='train'):\n",
    "        self.df = df\n",
    "        self.bsz = batch_size\n",
    "        self.mode = mode\n",
    "\n",
    "        # Take labels and a list of image locations in memory\n",
    "        self.labels = self.df['categ_number'].values\n",
    "        self.im_list = self.df['path'].tolist()\n",
    "        #self.des = self.df['tsne'].tolist()\n",
    "        self.des = self.df.iloc[:,4:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(math.ceil(len(self.df) / float(self.bsz)))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Shuffles indexes after each epoch if in training mode\n",
    "        self.indexes = range(len(self.im_list))\n",
    "        if self.mode == 'train':\n",
    "            self.indexes = random.sample(self.indexes, k=len(self.indexes))\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return self.labels[idx * self.bsz: (idx + 1) * self.bsz]\n",
    "\n",
    "    def get_batch_features(self, idx, type_=\"img\"):\n",
    "        # Fetch a batch of inputs\n",
    "        if type_ == \"img\":\n",
    "            ret = np.array([load_image(im) for im in self.im_list[idx * self.bsz: (1 + idx) * self.bsz]])\n",
    "        else:\n",
    "            #ret = np.array([tf for tf in self.des[idx * self.bsz: (1 + idx) * self.bsz]])\n",
    "            ret = np.array([tf[1].values for tf in self.des[idx * self.bsz: (1 + idx) * self.bsz].iterrows()])\n",
    "        return ret\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.get_batch_features(idx, type_=\"img\")\n",
    "        batch_x2 = self.get_batch_features(idx, type_=\"txt\")\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        return [batch_x, batch_x2], [batch_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, Dense, Dropout, MaxPool2D, Flatten, concatenate\n",
    "\n",
    "inputx = Input(shape=(224,224,3))\n",
    "x = MaxPool2D()(Conv2D(32, (3,3), activation='relu')(inputx))\n",
    "x = MaxPool2D()(Conv2D(32, (3,3), activation='relu')(x))\n",
    "x = MaxPool2D()(Conv2D(32, (3,3), activation='relu')(x))\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.2)(Dense(32, activation='relu')(x))\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dense(4, activation='relu')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "inputx2= Input(shape=(5718,), dtype='float32', name='tfidf_input')\n",
    "x2 = Dense(32, activation='relu')(inputx2)\n",
    "x2 = Dense(4, activation='relu')(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedInput = concatenate([x, x2])\n",
    "\n",
    "comb = Dense(4, activation='relu')(combinedInput)\n",
    "comb = Dense(7, activation='softmax')(comb)\n",
    "\n",
    "model = Model(inputs=[inputx, inputx2], outputs=comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=1e-5, clipnorm = 1.)\n",
    "loss_fn  = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(loss=loss_fn, optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 222, 222, 32) 896         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 111, 111, 32) 0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 109, 109, 32) 9248        max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 54, 54, 32)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 52, 52, 32)   9248        max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 26, 26, 32)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 21632)        0           max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 32)           692256      flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32)           0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tfidf_input (InputLayer)        (None, 5718)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 32)           1056        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 32)           183008      tfidf_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 4)            132         dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 4)            132         dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 8)            0           dense_23[0][0]                   \n",
      "                                                                 dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 4)            36          concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 7)            35          dense_24[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 896,047\n",
      "Trainable params: 896,047\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_model[data_model['label'] == 'Train']\n",
    "val = data_model[data_model['label'] == 'Validation']\n",
    "test = data_model[data_model['label'] == 'Test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20/20 [==============================] - 18s 886ms/step - loss: 1.9435 - accuracy: 0.1206 - val_loss: 1.9318 - val_accuracy: 0.0619\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 15s 771ms/step - loss: 1.9433 - accuracy: 0.1857 - val_loss: 1.9313 - val_accuracy: 0.0460\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 15s 752ms/step - loss: 1.9422 - accuracy: 0.1143 - val_loss: 1.9290 - val_accuracy: 0.0413\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 15s 760ms/step - loss: 1.9425 - accuracy: 0.1730 - val_loss: 1.9276 - val_accuracy: 0.2556\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 15s 771ms/step - loss: 1.9421 - accuracy: 0.1667 - val_loss: 1.9316 - val_accuracy: 0.0222\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 16s 778ms/step - loss: 1.9417 - accuracy: 0.0397 - val_loss: 1.9277 - val_accuracy: 0.0238\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 16s 778ms/step - loss: 1.9415 - accuracy: 0.1508 - val_loss: 1.9272 - val_accuracy: 0.1206\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 16s 775ms/step - loss: 1.9408 - accuracy: 0.0873 - val_loss: 1.9269 - val_accuracy: 0.0667\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 15s 769ms/step - loss: 1.9396 - accuracy: 0.0921 - val_loss: 1.9220 - val_accuracy: 0.0619\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 16s 776ms/step - loss: 1.9392 - accuracy: 0.0651 - val_loss: 1.9251 - val_accuracy: 0.0556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x253620474c8>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = DataSequence(train,  batch_size=32)\n",
    "valid = DataSequence(train,  batch_size=32, mode='Val')\n",
    "model.fit_generator(seq, epochs=10, validation_data=valid, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
