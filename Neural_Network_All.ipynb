{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load data</a></span></li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Normalisation\" data-toc-modified-id=\"Normalisation-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Normalisation</a></span></li><li><span><a href=\"#Stemming\" data-toc-modified-id=\"Stemming-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Stemming</a></span></li><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>TF-IDF</a></span></li></ul></li><li><span><a href=\"#Merging-Dataframes\" data-toc-modified-id=\"Merging-Dataframes-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Merging Dataframes</a></span></li><li><span><a href=\"#Building-generators\" data-toc-modified-id=\"Building-generators-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Building generators</a></span></li><li><span><a href=\"#Model-Creation\" data-toc-modified-id=\"Model-Creation-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Model Creation</a></span><ul class=\"toc-item\"><li><span><a href=\"#CNN\" data-toc-modified-id=\"CNN-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>CNN</a></span></li><li><span><a href=\"#VGG16-(use-either-this-or-CNN)\" data-toc-modified-id=\"VGG16-(use-either-this-or-CNN)-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>VGG16 (use either this or CNN)</a></span></li><li><span><a href=\"#LSTM\" data-toc-modified-id=\"LSTM-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>LSTM</a></span></li><li><span><a href=\"#Combining-the-output\" data-toc-modified-id=\"Combining-the-output-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Combining the output</a></span></li><li><span><a href=\"#Compilation\" data-toc-modified-id=\"Compilation-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Compilation</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Training</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\__init__.py:29: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/data.csv\")\n",
    "data = data[['uniq_id', 'description', 'categ', 'path', 'label']].set_index('uniq_id')\n",
    "\n",
    "# We transform the categ into numeric values\n",
    "data.categ = pd.Categorical(data.categ)\n",
    "data['categ_number'] = np.int32(data.categ.cat.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>categ</th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "      <th>categ_number</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uniq_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4b500e244f11a45f5171bd3665413044</th>\n",
       "      <td>Key Features of Jacadi paris Baby Boy's Stripe...</td>\n",
       "      <td>Baby_Care</td>\n",
       "      <td>data/Train/Baby_Care/4b500e244f11a45f5171bd366...</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7ee2e665b7de2e1cc29e3c65bde0c318</th>\n",
       "      <td>Buy Mavi 237MBB Showpiece  -  15 cm for Rs.129...</td>\n",
       "      <td>Home_Decor_And_Festive_Needs</td>\n",
       "      <td>data/Train/Home_Decor_And_Festive_Needs/7ee2e6...</td>\n",
       "      <td>Train</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d21409978f5e404b53f89e444f7893fe</th>\n",
       "      <td>SF by Sonata 7991PP02 Ocean Digital Watch  - F...</td>\n",
       "      <td>Watches</td>\n",
       "      <td>data/Train/Watches/d21409978f5e404b53f89e444f7...</td>\n",
       "      <td>Train</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47d7792e50e69b048b1f17176f170141</th>\n",
       "      <td>Flipkart.com: Buy Beverly Hills Polo Club RED ...</td>\n",
       "      <td>Beauty_and_Personal_Care</td>\n",
       "      <td>data/Train/Beauty_and_Personal_Care/47d7792e50...</td>\n",
       "      <td>Train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ecb5bba9a5984d2b35b8ac3f56056c67</th>\n",
       "      <td>Flipkart.com: Buy Wild Stone Red And Juice Com...</td>\n",
       "      <td>Beauty_and_Personal_Care</td>\n",
       "      <td>data/Train/Beauty_and_Personal_Care/ecb5bba9a5...</td>\n",
       "      <td>Train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        description  \\\n",
       "uniq_id                                                                               \n",
       "4b500e244f11a45f5171bd3665413044  Key Features of Jacadi paris Baby Boy's Stripe...   \n",
       "7ee2e665b7de2e1cc29e3c65bde0c318  Buy Mavi 237MBB Showpiece  -  15 cm for Rs.129...   \n",
       "d21409978f5e404b53f89e444f7893fe  SF by Sonata 7991PP02 Ocean Digital Watch  - F...   \n",
       "47d7792e50e69b048b1f17176f170141  Flipkart.com: Buy Beverly Hills Polo Club RED ...   \n",
       "ecb5bba9a5984d2b35b8ac3f56056c67  Flipkart.com: Buy Wild Stone Red And Juice Com...   \n",
       "\n",
       "                                                         categ  \\\n",
       "uniq_id                                                          \n",
       "4b500e244f11a45f5171bd3665413044                     Baby_Care   \n",
       "7ee2e665b7de2e1cc29e3c65bde0c318  Home_Decor_And_Festive_Needs   \n",
       "d21409978f5e404b53f89e444f7893fe                       Watches   \n",
       "47d7792e50e69b048b1f17176f170141      Beauty_and_Personal_Care   \n",
       "ecb5bba9a5984d2b35b8ac3f56056c67      Beauty_and_Personal_Care   \n",
       "\n",
       "                                                                               path  \\\n",
       "uniq_id                                                                               \n",
       "4b500e244f11a45f5171bd3665413044  data/Train/Baby_Care/4b500e244f11a45f5171bd366...   \n",
       "7ee2e665b7de2e1cc29e3c65bde0c318  data/Train/Home_Decor_And_Festive_Needs/7ee2e6...   \n",
       "d21409978f5e404b53f89e444f7893fe  data/Train/Watches/d21409978f5e404b53f89e444f7...   \n",
       "47d7792e50e69b048b1f17176f170141  data/Train/Beauty_and_Personal_Care/47d7792e50...   \n",
       "ecb5bba9a5984d2b35b8ac3f56056c67  data/Train/Beauty_and_Personal_Care/ecb5bba9a5...   \n",
       "\n",
       "                                  label  categ_number  \n",
       "uniq_id                                                \n",
       "4b500e244f11a45f5171bd3665413044  Train             0  \n",
       "7ee2e665b7de2e1cc29e3c65bde0c318  Train             3  \n",
       "d21409978f5e404b53f89e444f7893fe  Train             6  \n",
       "47d7792e50e69b048b1f17176f170141  Train             1  \n",
       "ecb5bba9a5984d2b35b8ac3f56056c67  Train             1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will apply some function to the description for modeling later on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = data.reset_index()\n",
    "data_text = data_text[['categ', 'description']]\n",
    "# We take one line per category and sum the description\n",
    "data_text = data_text.groupby('categ').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal here is to extract the frequences of the words\n",
    "# And also to extract the frequences of unique words\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "desc = defaultdict(list)\n",
    "\n",
    "for i in data_text.index:\n",
    "    desc[i] += tokenizer.tokenize(data_text.loc[i, 'description'].lower())\n",
    "\n",
    "stats, freq = dict(), dict()\n",
    "\n",
    "for k, v in zip(desc.keys(), desc.values()):\n",
    "    freq[k] = fq = nltk.FreqDist(v)\n",
    "    stats[k] = {'total': len(v), 'Unique': len(fq.keys())} \n",
    "\n",
    "df = pd.DataFrame.from_dict(stats, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we get the freq of words on our dataset\n",
    "freq_totale = nltk.Counter()\n",
    "for k, v in zip(desc.keys(), desc.values()):\n",
    "    freq_totale += freq[k]\n",
    "\n",
    "# Here is our list of stop words\n",
    "most_freq = { key:value for (key,value) in freq_totale.items() if value >= 100}.keys()\n",
    "\n",
    "# Let's add the common stop words list of nltk\n",
    "sw = set()\n",
    "sw.update(most_freq)\n",
    "sw.update(tuple(nltk.corpus.stopwords.words('english')))\n",
    "sw.update(tuple(['!', '?', '.', ';', '\"', \"'\", \"#\",\n",
    "                 \"&\", \"/\", \"-\", \"_\", \"'m\",\n",
    "                 \"%\", \"*\", \"(\", \")\", \"±3\",\n",
    "                 \",\", \"'s\", '“', '”']))\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "desc = defaultdict(list)\n",
    "\n",
    "for i in data_text.index:\n",
    "    token = tokenizer.tokenize(data_text.loc[i, 'description'].lower())\n",
    "    desc[i] += [w for w in token if not w in list(sw)]\n",
    "\n",
    "stats, freq = dict(), dict()\n",
    "\n",
    "for k, v in zip(desc.keys(), desc.values()):\n",
    "    freq[k] = fq = nltk.FreqDist(v)\n",
    "    stats[k] = {'total': len(v), 'Unique': len(fq.keys())} \n",
    "\n",
    "df = pd.DataFrame.from_dict(stats, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = EnglishStemmer()\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "desc = defaultdict(list)\n",
    "\n",
    "for i in data_text.index:\n",
    "    token = tokenizer.tokenize(data_text.loc[i, 'description'].lower())\n",
    "    desc[i] += [stemmer.stem(w) for w in token if not w in list(sw)]\n",
    "\n",
    "stats, freq = dict(), dict()\n",
    "\n",
    "for k, v in zip(desc.keys(), desc.values()):\n",
    "    freq[k] = fq = nltk.FreqDist(v)\n",
    "    stats[k] = {'total': len(v), 'Unique': len(fq.keys())}\n",
    "\n",
    "df = pd.DataFrame.from_dict(stats, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", '``', 'abov', 'adapt', 'ani', 'babi', 'batteri', 'becaus', 'befor', 'ceram', 'coffe', 'content', 'could', 'deliveri', 'detail', 'dimens', 'discount', 'doe', 'dure', 'easi', 'featur', 'genuin', 'guarante', 'materi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'onlin', 'ourselv', 'packag', 'qualiti', 'replac', 'sale', 'sha', 'ship', 'showpiec', 'specif', 'themselv', 'veri', 'warranti', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "data_tf = data.reset_index()[['uniq_id', 'categ', 'description']]\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "token_dict = defaultdict(list)\n",
    "\n",
    "for i in range(0, len(data_tf)):\n",
    "    lowers = data_tf.loc[i,'description'].lower()\n",
    "    no_punctuation = lowers.translate(string.punctuation)\n",
    "    token_dict[i] = no_punctuation\n",
    "\n",
    "tfIdf = TfidfVectorizer(tokenizer=tokenize, stop_words=sw)\n",
    "values = tfIdf.fit_transform(token_dict.values())\n",
    "\n",
    "#Transform the results into a df\n",
    "values = values.todense()\n",
    "values = pd.DataFrame(values, index=data.index)\n",
    "X_embedded = TSNE(n_components=1).fit_transform(values)\n",
    "X_embedded_df = pd.DataFrame(X_embedded, index=data.index, columns=['tsne'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model = pd.merge(data, values,\n",
    "                      left_index=True, right_index=True,\n",
    "                      how='inner')\n",
    "data_model = data_model.drop(columns=['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the code can be found here : https://github.com/sdcubber/Keras-Sequence-boilerplate/blob/master/Keras-Sequence.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import math\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import Sequence\n",
    "from PIL import Image\n",
    "\n",
    "def load_image(im):\n",
    "    return img_to_array(load_img(im, target_size=(224, 224), grayscale=False)) / 255.\n",
    "\n",
    "class DataSequence(Sequence):\n",
    "    \"\"\"\n",
    "    Keras Sequence object to train a model on larger-than-memory data.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, batch_size, mode='train'):\n",
    "        self.df = df\n",
    "        self.bsz = batch_size\n",
    "        self.mode = mode\n",
    "\n",
    "        # Take labels and a list of image locations in memory\n",
    "        self.labels = self.df['categ_number'].values\n",
    "        self.im_list = self.df['path'].tolist()\n",
    "        #self.des = self.df['tsne'].tolist()\n",
    "        self.des = self.df.iloc[:,4:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(math.ceil(len(self.df) / float(self.bsz)))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Shuffles indexes after each epoch if in training mode\n",
    "        self.indexes = range(len(self.im_list))\n",
    "        if self.mode == 'train':\n",
    "            self.indexes = random.sample(self.indexes, k=len(self.indexes))\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return self.labels[idx * self.bsz: (idx + 1) * self.bsz]\n",
    "\n",
    "    def get_batch_features(self, idx, type_=\"img\"):\n",
    "        # Fetch a batch of inputs\n",
    "        if type_ == \"img\":\n",
    "            ret = np.array([load_image(im) for im in self.im_list[idx * self.bsz: (1 + idx) * self.bsz]])\n",
    "        else:\n",
    "            #ret = np.array([tf for tf in self.des[idx * self.bsz: (1 + idx) * self.bsz]])\n",
    "            ret = np.array([tf[1].values for tf in self.des[idx * self.bsz: (1 + idx) * self.bsz].iterrows()])\n",
    "        return ret\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.get_batch_features(idx, type_=\"img\")\n",
    "        batch_x2 = self.get_batch_features(idx, type_=\"txt\")\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        return [batch_x, batch_x2], [batch_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, Dense, Dropout, MaxPool2D, Flatten, concatenate\n",
    "\n",
    "# inputx = Input(shape=(224,224,3))\n",
    "# x = MaxPool2D()(Conv2D(32, (3,3), activation='relu')(inputx))\n",
    "# x = MaxPool2D()(Conv2D(32, (3,3), activation='relu')(x))\n",
    "# x = MaxPool2D()(Conv2D(32, (3,3), activation='relu')(x))\n",
    "# x = Flatten()(x)\n",
    "# x = Dropout(0.2)(Dense(32, activation='relu')(x))\n",
    "# x = Dense(32, activation='relu')(x)\n",
    "# x = Dense(9, activation='relu')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 (use either this or CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll here do some transfer learning with VGG16 <br/>\n",
    "As the number of data that we have is low, we'll not retrain the Convolution part of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height, img_width = 224,224\n",
    "inputx = Input(shape=(224, 224, 3), name='image')\n",
    "vgg16 = VGG16(weights='imagenet', include_top=False,\n",
    "              input_shape = (img_width, img_height, 3))(inputx)\n",
    "x = Flatten()(vgg16) #Flatten the output of vgg16\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first tries, we'll not add LSTM layers on the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "inputx2= Input(shape=(5718,), dtype='float32', name='tfidf_input')\n",
    "x2 = Dense(512, activation='relu')(inputx2)\n",
    "x2 = Dropout(0.15)(x2)\n",
    "x2 = Dense(256, activation='relu')(x2)\n",
    "x2 = Dense(64, activation='relu')(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedInput = concatenate([x, x2])\n",
    "\n",
    "comb = Dense(9, activation='relu')(combinedInput)\n",
    "comb = Dense(7, activation='softmax')(comb)\n",
    "\n",
    "model = Model(inputs=[inputx, inputx2], outputs=comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=1e-5, clipnorm = 1.)\n",
    "loss_fn  = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(loss=loss_fn, optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image (InputLayer)              (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tfidf_input (InputLayer)        (None, 5718)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vgg16 (Model)                   (None, 7, 7, 512)    14714688    image[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 512)          2928128     tfidf_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 25088)        0           vgg16[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 256)          6422784     flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 256)          131328      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 64)           16448       dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 64)           16448       dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128)          0           dense_8[0][0]                    \n",
      "                                                                 dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 9)            1161        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 7)            70          dense_12[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 24,231,055\n",
      "Trainable params: 24,231,055\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_model[data_model['label'] == 'Train']\n",
    "val = data_model[data_model['label'] == 'Validation']\n",
    "test = data_model[data_model['label'] == 'Test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "20/20 [==============================] - 38s 2s/step - loss: 1.9088 - accuracy: 0.0016 - val_loss: 1.8831 - val_accuracy: 0.0111\n",
      "Epoch 2/30\n",
      "20/20 [==============================] - 24s 1s/step - loss: 1.7654 - accuracy: 0.0079 - val_loss: 1.7158 - val_accuracy: 0.0016\n",
      "Epoch 3/30\n",
      "20/20 [==============================] - 32s 2s/step - loss: 1.6052 - accuracy: 0.0397 - val_loss: 1.4912 - val_accuracy: 0.1762\n",
      "Epoch 4/30\n",
      "20/20 [==============================] - 22s 1s/step - loss: 1.4864 - accuracy: 0.1794 - val_loss: 1.3640 - val_accuracy: 0.1762\n",
      "Epoch 5/30\n",
      "20/20 [==============================] - 16s 808ms/step - loss: 1.4266 - accuracy: 0.2143 - val_loss: 1.3193 - val_accuracy: 0.1841\n",
      "Epoch 6/30\n",
      "20/20 [==============================] - 27s 1s/step - loss: 1.4012 - accuracy: 0.1810 - val_loss: 1.3219 - val_accuracy: 0.2063\n",
      "Epoch 7/30\n",
      "20/20 [==============================] - 27s 1s/step - loss: 1.3775 - accuracy: 0.1746 - val_loss: 1.3418 - val_accuracy: 0.1460\n",
      "Epoch 8/30\n",
      "20/20 [==============================] - 16s 797ms/step - loss: 1.3546 - accuracy: 0.1524 - val_loss: 1.2855 - val_accuracy: 0.1429\n",
      "Epoch 9/30\n",
      "20/20 [==============================] - 28s 1s/step - loss: 1.3202 - accuracy: 0.1413 - val_loss: 1.2412 - val_accuracy: 0.1460\n",
      "Epoch 10/30\n",
      "20/20 [==============================] - 16s 823ms/step - loss: 1.2994 - accuracy: 0.1429 - val_loss: 1.2238 - val_accuracy: 0.1429\n",
      "Epoch 11/30\n",
      "20/20 [==============================] - 32s 2s/step - loss: 1.2808 - accuracy: 0.1286 - val_loss: 1.2405 - val_accuracy: 0.1413\n",
      "Epoch 12/30\n",
      "20/20 [==============================] - 39s 2s/step - loss: 1.2704 - accuracy: 0.1317 - val_loss: 1.2148 - val_accuracy: 0.1349\n",
      "Epoch 13/30\n",
      "20/20 [==============================] - 18s 900ms/step - loss: 1.2568 - accuracy: 0.1270 - val_loss: 1.2220 - val_accuracy: 0.1254\n",
      "Epoch 14/30\n",
      "20/20 [==============================] - 32s 2s/step - loss: 1.2424 - accuracy: 0.1302 - val_loss: 1.2103 - val_accuracy: 0.1381\n",
      "Epoch 15/30\n",
      "20/20 [==============================] - 17s 841ms/step - loss: 1.2415 - accuracy: 0.1413 - val_loss: 1.2094 - val_accuracy: 0.1381\n",
      "Epoch 16/30\n",
      "20/20 [==============================] - 51s 3s/step - loss: 1.2392 - accuracy: 0.1286 - val_loss: 1.2094 - val_accuracy: 0.1381\n",
      "Epoch 17/30\n",
      "20/20 [==============================] - 28s 1s/step - loss: 1.2322 - accuracy: 0.1381 - val_loss: 1.2096 - val_accuracy: 0.1397\n",
      "Epoch 18/30\n",
      "20/20 [==============================] - 36s 2s/step - loss: 1.2288 - accuracy: 0.1413 - val_loss: 1.1666 - val_accuracy: 0.1349\n",
      "Epoch 19/30\n",
      "20/20 [==============================] - 22s 1s/step - loss: 1.2302 - accuracy: 0.1413 - val_loss: 1.2103 - val_accuracy: 0.1603\n",
      "Epoch 20/30\n",
      "20/20 [==============================] - 20s 997ms/step - loss: 1.2486 - accuracy: 0.1317 - val_loss: 1.2075 - val_accuracy: 0.1492\n",
      "Epoch 21/30\n",
      "20/20 [==============================] - 34s 2s/step - loss: 1.2225 - accuracy: 0.1460 - val_loss: 1.1660 - val_accuracy: 0.1476\n",
      "Epoch 22/30\n",
      "20/20 [==============================] - 44s 2s/step - loss: 1.2158 - accuracy: 0.1460 - val_loss: 1.1712 - val_accuracy: 0.1381\n",
      "Epoch 23/30\n",
      "20/20 [==============================] - 29s 1s/step - loss: 1.2221 - accuracy: 0.1413 - val_loss: 1.1696 - val_accuracy: 0.1460\n",
      "Epoch 24/30\n",
      "20/20 [==============================] - 27s 1s/step - loss: 1.2186 - accuracy: 0.1429 - val_loss: 1.1659 - val_accuracy: 0.1413\n",
      "Epoch 25/30\n",
      "20/20 [==============================] - 21s 1s/step - loss: 1.2129 - accuracy: 0.1429 - val_loss: 1.1690 - val_accuracy: 0.1429\n",
      "Epoch 26/30\n",
      "20/20 [==============================] - 23s 1s/step - loss: 1.2123 - accuracy: 0.1444 - val_loss: 1.1669 - val_accuracy: 0.1381\n",
      "Epoch 27/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.2061 - accuracy: 0.1413 - val_loss: 1.1654 - val_accuracy: 0.1429\n",
      "Epoch 28/30\n",
      "20/20 [==============================] - 17s 871ms/step - loss: 1.2043 - accuracy: 0.1413 - val_loss: 1.1660 - val_accuracy: 0.1444\n",
      "Epoch 29/30\n",
      "20/20 [==============================] - 27s 1s/step - loss: 1.2049 - accuracy: 0.1413 - val_loss: 1.1885 - val_accuracy: 0.1381\n",
      "Epoch 30/30\n",
      "20/20 [==============================] - 17s 841ms/step - loss: 1.2058 - accuracy: 0.1413 - val_loss: 1.1658 - val_accuracy: 0.1429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2531654bfc8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = DataSequence(train,  batch_size=32)\n",
    "valid = DataSequence(train,  batch_size=32, mode='Val')\n",
    "model.fit_generator(seq, epochs=30, validation_data=valid, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
